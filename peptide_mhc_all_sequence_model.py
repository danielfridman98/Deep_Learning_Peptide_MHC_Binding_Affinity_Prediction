# -*- coding: utf-8 -*-
"""Peptide_MHC_all_sequence_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DxbFpJPeN70lBvrgoaMz9SxzaDTSSznX
"""

# %cd /content/drive/My Drive/Deep_Learning_Final_Project/protein-sequence-embedding-iclr2019-master
from google.colab import drive
drive.mount('/content/drive')
import os
os.chdir('/content/drive/My Drive/Deep_Learning_Final_Project/protein-sequence-embedding-iclr2019-master')



!pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html

import torch
import torch.nn as nn 
import torch.nn.functional as F
import pandas as pd
import numpy as np
from peptide_embedding import *  
from MHC_sequence_embedding import *
from random import sample
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

torch.cuda.get_device_name(0)

# Neural network model for processing peptide sequence embeddings and MHC allele embeddings
# and predicting binary (0 or 1) binding affinity as output

# Peptide and MHC pre-trained amino acid sequence embeddings as inputs
# Input embedding dimension = (batch_size * sequence_length * embedding_size)

# Model includes 3 components:
    # Component 1: peptide sequence embedding processing layer - bidirectional GRU
    # Component 2: MHC allele sequence embedding processing layer - bidirectional GRU
    # Component 3: Concatenation of hidden layers from components 1 and 2 and binding affinity prediction - 3 Feed Forward layers
class peptide_BiGRU(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(peptide_BiGRU, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        self.peptide_bidir_gru =  nn.GRU(input_size, hidden_size, batch_first=True, bidirectional = True)
        
    def forward(self, x):
        gru_output, _ = self.peptide_bidir_gru(x) # gru_output: tensor of shape (batch_size, seq_length, hidden_size*2)
        # gru_output = torch.cat((gru_output[:,0,:], gru_output[:,-1,:]), dim=1) # concatenate hidden layer for first and latest time step
        gru_output = torch.cat((gru_output[:,0,:], gru_output[:,-1,:]), dim=1)
        return gru_output
    
class allele_BiGRU(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(allele_BiGRU, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        self.allele_bidir_gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)
        
    def forward(self, x): 
        gru_output, _ = self.allele_bidir_gru(x)
        gru_output = torch.cat((gru_output[:,0,:], gru_output[:,-1,:]), dim=1)
        return gru_output 
    
class Output_Layer(nn.Module):
    def __init__(self, peptide_BiGRU_model, allele_BiGRU_model, peptide_dim, allele_dim):
        super(Output_Layer, self).__init__()
        self.peptide_model = peptide_BiGRU_model
        self.allele_model = allele_BiGRU_model
        
        self.Hidden1 = nn.Linear((peptide_dim + allele_dim), 200)
        self.Hidden2 = nn.Linear(200, 200)
        self.output = nn.Linear(200,1)
        self.relu = nn.ReLU()
        
    def forward(self, peptide_input, allele_input):
        peptide_output = self.peptide_model(peptide_input)
        allele_output = self.allele_model(allele_input)
        
        input_concat = torch.cat((peptide_output, allele_output), dim=1)
        Hidden1 = self.relu(self.Hidden1(input_concat))
        Hidden2 = self.relu(self.Hidden2(Hidden1))
        output = torch.sigmoid(self.output(Hidden2))
        return output

# Full Model w/MHC embedding

# Load Dataset 
# Dataset of peptide sequence, MHC allele name, binary binding affinity (positive, negative)
link1 = 'https://raw.githubusercontent.com/cmb-chula/MHCSeqNet/master/cleaned_MHC_all_classes.csv'
x = pd.read_csv(link1)

# Dataset of corresponding amino acid sequence for MHC alleles (Beta sheet, alpha helix res 140-179, alpha helix res 50-84)
link2 = 'https://raw.githubusercontent.com/cmb-chula/MHCSeqNet/master/PretrainedModels/sequence_model/AlleleInformation.txt'
allele_seq = urllib.request.urlopen(link2)
MHC_sequence_df = MHC_seq_df(allele_seq)

# Process and randomize data
alleles = x['allele'] 
good_idx = alleles.isin(MHC_sequence_df['MHC_allele'])
# classI_alleles = alleles[good_idx]

# peptides = x['peptide'] 
# peptides = peptides[good_idx]

binding_affinity = x['binding_quality']
binding_affinity = binding_affinity[good_idx]

del link1
del x
del link2
del allele_seq
del MHC_sequence_df

print(len(binding_affinity))



# Gets distribution of peptide lengths
link1 = 'https://raw.githubusercontent.com/cmb-chula/MHCSeqNet/master/cleaned_MHC_all_classes.csv'
x = pd.read_csv(link1)
peptides = x['peptide'].to_list()
lens = [len(i) for i in peptides]
unique_lens = []
for length in lens:
  if length not in unique_lens:
    unique_lens.append(length)

counts = {}
fracs = {}
for length in sorted(unique_lens):
  counts[length] = lens.count(length)
  fracs[length] = round(lens.count(length) / len(peptides), 3)
print(counts)
print(fracs)

# Load Embedding tensors
# Allele tensors
# alpha_140_179_embedding_tensor = torch.load('alpha_140_179_part_data.pt')
alpha_140_179_embedding_tensor = torch.load('data_tensors/alpha_140_179_full_data2.pt')
print('1/3')
# alpha_50_84_embedding_tensor = torch.load('alpha_50_84_part_data.pt')
alpha_50_84_embedding_tensor = torch.load('data_tensors/alpha_50_84_full_data2.pt')
print('2/3')

allele_embedding_tensor = torch.cat((alpha_140_179_embedding_tensor, alpha_50_84_embedding_tensor), dim=1)

del alpha_140_179_embedding_tensor
del alpha_50_84_embedding_tensor
print(allele_embedding_tensor.shape)

# Peptide tensor
# peptide_embedding_tensor = torch.load('peptide_part_data.pt')
peptide_embedding_tensor = torch.load('data_tensors/peptide_full_data2.pt')
print('3/3')

# Randomize tensors
# random index
rand_idx = np.random.permutation(allele_embedding_tensor.shape[0])

# randomize allele tensors
# rand_140_179_tensor = alpha_140_179_embedding_tensor[rand_idx,:,:]
# rand_50_84_tensor = alpha_50_84_embedding_tensor[rand_idx,:,:]
allele_embedding_tensor = allele_embedding_tensor[rand_idx,:,:]

# randomize peptide tensor
peptide_embedding_tensor = peptide_embedding_tensor[rand_idx,:,:]

# randomize target binding affinity
# binding_affinity = binding_affinity[rand_idx]
# binding_affinity = binding_affinity.values.tolist()
# y = np.array(binding_affinity_class(binding_affinity.values.tolist()))[:150000][rand_idx]
y = np.array(binding_affinity_class(binding_affinity.values.tolist()))[rand_idx]
print(len(y))
# rand_y = y[rand_idx]

# Create train and test sets
train_len = round(0.80*peptide_embedding_tensor.shape[0])

train_peptide = peptide_embedding_tensor[:train_len,:,:]
# test_peptide = peptide_embedding_tensor[train_len:,:,:]

train_allele = allele_embedding_tensor[:train_len,:,:]
# test_allele = allele_embedding_tensor[train_len:,:,:]

# train_allele_140_179 = rand_140_179_tensor[:train_len,:,:]
# test_allele_140_179 = rand_140_179_tensor[train_len:,:,:]

# train_allele_50_84 = rand_50_84_tensor[:train_len,:,:]
# test_allele_50_84 = rand_50_84_tensor[train_len:,:,:]

train_y = y[:train_len]
# test_y = y[train_len:]

# Train 
n_epochs = 200
batch_size = 32

peptide_embedding_dim = 100
peptide_hidden_dim = 100

allele_embedding_dim = 100
allele_hidden_dim = 100

# peptide_dim = peptide_hidden_size*2*2
# allele_dim = allele_hidden_size*2*2 
# (b/c bidirectional (hidden*2) and 1st and last layers concatenated (hidden*2*2))
peptide_to_output_dim = peptide_hidden_dim*2*2
allele_to_output_dim = allele_hidden_dim*2*2

# Define Model
peptide_model = peptide_BiGRU(peptide_embedding_dim, peptide_hidden_dim).cuda()
allele_model = allele_BiGRU(allele_embedding_dim, allele_hidden_dim).cuda()
output_model = Output_Layer(peptide_model, allele_model, peptide_to_output_dim, allele_to_output_dim).cuda()

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(output_model.parameters())





# Train model 
for epoch in range(n_epochs):
    epoch_loss = 0
    batch_count = 0
    avg_epoch_acc = 0

    # randomly shuffle data tensors by rand_idx
    # rand_idx = np.random.permutation(train_peptide.shape[0])
    # train_peptide = train_peptide[rand_idx,:,:]
    # test_peptide = test_peptide[rand_idx,:,:]

    # train_allele_140_179 = train_allele_140_179[rand_idx,:,:]
    # test_allele_140_179 = test_allele_140_179[rand_idx,:,:]

    # train_allele_50_84 = train_allele_50_84[rand_idx,:,:]
    # test_allele_50_84 = test_allele_50_84[rand_idx,:,:]

    # randomly shuffle target y by rand_idx
    # train_y = train_y[rand_idx]
    # test_y = test_y[rand_idx]

    for i in range(0, train_len, batch_size):
    # for i in range(0,32*1000,batch_size):
      batch_count += 1

      # Run model, generate y_hat predictions, compute loss
      y_hat = output_model(train_peptide[i:i+batch_size].cuda(), train_allele[i:i+batch_size].cuda())
      y_batch = torch.tensor(train_y[i:i+batch_size]).cuda().float()
      batch_loss = criterion(y_hat.squeeze(), y_batch)
      epoch_loss += float(batch_loss)
        
      # Update weights using Adam optimization
      optimizer.zero_grad()
      batch_loss.backward()
      optimizer.step()
        
      # Compute accuracy
      acc = float((abs(y_batch - y_hat.squeeze()) < .5).sum()) / batch_size
      avg_epoch_acc += acc
      if batch_count % 100 == 0:
          print('batch: ', batch_count)
          print("batch_loss: {:.4f}, Accuracy: {:.4f}".format(batch_loss, acc))

    avg_epoch_acc = avg_epoch_acc/batch_count
    print('epoch: ', epoch)
    print("epoch_loss: ", round(epoch_loss,4), ", Avg epoch accuracy: ", round(avg_epoch_acc,4))


torch.save(output_model.state_dict(), '/content/drive/My Drive/Deep_Learning_Final_Project/protein-sequence-embedding-iclr2019-master/trained_peptide_allele_sequence_full_model_200_epoch.pt')

# Test
test_len = peptide_embedding_tensor.shape[0] - train_len
test_peptide = peptide_embedding_tensor[train_len:,:,:].cuda()
test_allele = allele_embedding_tensor[train_len:,:,:].cuda()
test_y = torch.tensor(y[train_len:]).cuda().float()

# Generate allele embedding test  
# test_allele = torch.cat((test_allele_140_179, test_allele_50_84), dim=1).cuda()
test_loss = 0
test_acc = 0
batch_count = 0
for i in range(0, test_len, batch_size):
  batch_count += 1
  test_y_hat =  output_model(test_peptide[i:i+batch_size], test_allele[i:i+batch_size])
  # test_y = torch.tensor(test_y).cuda().float()
  loss = criterion(test_y_hat.squeeze(), test_y[i:i+batch_size]) / batch_size
  test_loss += float(loss)
  test_acc += float((abs(test_y[i:i+batch_size] - test_y_hat.squeeze()) < .5).sum()) / batch_size

test_loss = test_loss / batch_count
test_acc = test_acc / batch_count
print("test_loss: {:.4f}, test_accuracy: {:.4f}".format(test_loss, test_acc))

from numpy import loadtxt
# load array
rand_idx = loadtxt('rand_idx.csv', delimiter=',')

allele_embedding_tensor = allele_embedding_tensor[rand_idx,:,:]

# randomize peptide tensor
peptide_embedding_tensor = peptide_embedding_tensor[rand_idx,:,:]

y = np.array(binding_affinity_class(binding_affinity.values.tolist()))[rand_idx.astype(int)]

# torch.save(output_model.state_dict(), '/content/drive/My Drive/Deep_Learning_Final_Project/protein-sequence-embedding-iclr2019-master/trained_peptide_allele_sequence_full_model.pt')

output_model.load_state_dict(torch.load('/content/drive/My Drive/Deep_Learning_Final_Project/protein-sequence-embedding-iclr2019-master/trained_peptide_allele_sequence_full_model_200_epoch.pt'))

output_model.eval()

# AUC Curve
test_loss = 0
test_acc = 0
batch_count = 0
all_y_hat = []

for i in range(0, test_len, batch_size):
  batch_count += 1
  test_y_hat =  output_model(test_peptide[i:i+batch_size], test_allele[i:i+batch_size])
  all_y_hat.append(test_y_hat.squeeze())
  # test_y = torch.tensor(test_y).cuda().float()
  loss = criterion(test_y_hat.squeeze(), test_y[i:i+batch_size]) / batch_size
  test_loss += float(loss)
  test_acc += float((abs(test_y[i:i+batch_size] - test_y_hat.squeeze()) < .5).sum()) / batch_size

test_loss = test_loss / batch_count
test_acc = test_acc / batch_count
print("test_loss: {:.4f}, test_accuracy: {:.4f}".format(test_loss, test_acc))

cat_y_hat = torch.cat(all_y_hat)

cat_y_hat = cat_y_hat.detach().cpu().numpy()
test_y = test_y.cpu().numpy()

# check all_y_hat, perhaps need to concatenate
fpr, tpr, _ = roc_curve(test_y, cat_y_hat)
roc_auc = auc(fpr, tpr)
print('roc_auc: ', roc_auc)

# Plot AUC curve
plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating Curve Peptide MHC Binding Affinity Prediction')
plt.legend(loc="lower right")
plt.show()



x = pd.read_csv('https://raw.githubusercontent.com/cmb-chula/MHCSeqNet/master/cleaned_MHC_all_classes.csv')
peptides = x['peptide'][good_idx].iloc[rand_idx][train_len:]
peptides = peptides.tolist()
lens = [len(i) for i in peptides]

unique_lens = np.unique(np.array(lens))

avg_predicted_binding_affinity = {}
avg_target_binding_affinity = {}
for length in unique_lens:
  idx = np.where(np.array(lens) == len)
  avg_predicted_binding_affinity[str(length) + '-mer'] = np.mean(cat_y_hat[np.where(np.array(lens) == length)])
  avg_target_binding_affinity[str(length) + '-mer'] = np.mean(test_y[np.where(np.array(lens) == length)])

print(avg_predicted_binding_affinity)
print(avg_target_binding_affinity)

print(len(np.where(test_y == 1)[0])/len(test_y))
print(len(np.where(test_y == 0)[0])/len(test_y))

print(len(np.where(train_y == 1)[0])/len(train_y))
print(len(np.where(train_y == 0)[0])/len(train_y))



